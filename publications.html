---
layout: default
tags: home
---
<h1 id="publications">Research</h1>
<table cellspacing="0" cellpadding="0" class = "worktable">
  <thead>
    <tr>
      <th><b><a href="https://link.springer.com/article/10.1007/s00521-022-07808-y">Who should I trust? Cautiously learning with unreliable experts</a></b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tamlin Love, Ritesh Ajoodha and Benjamin Rosman</td>
    </tr>
    <tr>
      <td><img src="img/lwtt.png" width="500"></td>
    </tr>
    <tr>
      <td><p>An important problem in reinforcement learning is the need for greater sample efficiency. One approach to dealing with this problem is to incorporate external information elicited from a domain expert in the learning process. Indeed, it has been shown that incorporating expert advice in the learning process can improve the rate at which an agent’s policy converges. However, these approaches typically assume a single, infallible expert; learning from multiple and/or unreliable experts is considered an open problem in assisted reinforcement learning. We present CLUE (cautiously learning with unreliable experts), a framework for learning single-stage decision problems with action advice from multiple, potentially unreliable experts that augments an unassisted learning with a model of expert reliability and a Bayesian method of pooling advice to select actions during exploration. Our results show that CLUE maintains the benefits of traditional approaches when advised by reliable experts, but is robust to the presence of unreliable experts. When learning with multiple experts, CLUE is able to rank experts by their reliability and differentiate experts based on their reliability.</p></td>
    </tr>
    <tr>
      <td></td>
    </tr>
    <tr>
      <td><i>Neural Computing and Applications, 2022</i></td>
    </tr>
    <tr>
      <td><a href="assets/Who_Should_I_Trust.pdf" target="new">Paper</a> · <a href="assets/Who_Should_I_Trust_supp.pdf" target="new">Supplementary Material</a> · <a href="https://github.com/tamlinlove/CLUE_SSDP" target="new">Code</a></td>
    </tr>
  </tbody>
</table>
<br>
<table cellspacing="0" cellpadding="0" class = "worktable">
  <thead>
    <tr>
      <th><b>Learning Who to Trust: Policy Learning in Single-Stage Decision Problems with Unreliable Expert Advice</b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tamlin Love, Ritesh Ajoodha and Benjamin Rosman</td>
    </tr>
    <tr>
      <td><img src="img/thesis_diagram.png" width="500"></td>
    </tr>
    <tr>
      <td><p>Work in the field of Assisted Reinforcement Learning (ARL) has shown that the incorporation of external
        information in problem solving can greatly increase the rate at which learners can converge to an optimal
        policy and aid in scaling algorithms to larger, more complex problems. However, these approaches rely
        on a single, reliable source of information; the problem of learning with information from multiple
        and/or unreliable sources of information is still an open question in ARL.We present CLUE (Cautiously
        Learning with Unreliable Experts), a framework for learning single-stage decision problems with policy
        advice from multiple, potentially unreliable experts. We compare CLUE against an unassisted agent and
        an agent that na&#207;vely follows advice, and our results show that CLUE exhibits faster convergence than
        an unassisted agent when advised by reliable experts, but is nevertheless robust against incorrect advice
        from unreliable experts.</p></td>
    </tr>
    <tr>
      <td></td>
    </tr>
    <tr>
      <td><i>2022, MSc Thesis</i></td>
    </tr>
    <tr>
      <td><a href="assets/thesis.pdf" target="new">Thesis</a> · <a href="https://github.com/tamlinlove/CLUE_SSDP" target="new">Code</a></td>
    </tr>
    <tr>
      <td>Work published in <i>Neural Computing and Applications 2022</i></td>
    </tr>
    <tr>
      <td>Work featured in <i>Multi-disciplinary Conference on Reinforcement Learning and Decision Making 2022</i> (<a href="assets/RLDM_Extended_Abstract.pdf" target="new">Extended Abstract</a> · <a href="assets/posters/RLDM_2022_Poster.pdf" target="new">Poster</a>)</td>
    </tr>
    <tr>
      <td>Work featured in <i>Workshop on Human-aligned Reinforcement Learning for Autonomous Agents and Robots 2021</i> (<a href="assets/Should I Trust You.pdf" target="new">Paper</a>)</td>
    </tr>
  </tbody>
</table>
<br>
<table cellspacing="0" cellpadding="0" class = "worktable">
<thead>
  <tr>
    <th><b>Building Undirected Influence Ontologies Using Pairwise Similarity Functions</b></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Tamlin Love and Ritesh Ajoodha</td>
  </tr>
  <tr>
    <td><img src="img/buio.png" width="500"></td>
  </tr>
  <tr>
    <td><p>The recovery of influence ontology structures is a
useful tool within knowledge discovery, allowing for an easy
and intuitive method of graphically representing the influences
between concepts or variables within a system. The focus of this
research is to develop a method by which undirected influence
structures, here in the form of undirected Bayesian network
skeletons, can be recovered from observations by means of
some pairwise similarity function, either a statistical measure
of correlation or some problem-specific measure.</p>
<p>In this research, we present two algorithms to construct
undirected influence structures from observations. The first
makes use of a threshold value to filter out relations denoting
weak influence, and the second constructs a maximum weighted
spanning tree over the complete set of relations. In addition, we
present a modification to the minimum graph edit distance (GED), which we refer to as the modified scaled GED, in order to
evaluate the performance of these algorithms in reconstructing
known structures. We perform a number of experiments in
reconstructing known Bayesian network structures, including a
real-world medical network. Our analysis shows that these
algorithms outperform a random reconstruction (modified scaled
GED ≈ 0.5), and can regularly achieve modified scaled GED
scores better than 0.3 in sparse cases and 0.45 in dense cases.</p>
<p>We argue that, while these methods cannot replace traditional
Bayesian network structure-learning techniques, they are useful
as computationally cheap data exploration tools and in knowledge
discovery over structures which cannot be modelled as Bayesian
networks.</p></td>
  </tr>
  <tr>
    <td></td>
  </tr>
  <tr>
    <td><i>2020 International SAUPEC/RobMech/PRASA Conference</i></td>
  </tr>
  <tr>
    <td><a href="assets/Building Undirected Influence Ontologies.pdf" target="new">Paper</a> · <a href="assets/posters/Honours_2019_Poster.pdf" target="new">Poster</a></td>
  </tr>
</tbody>
</table>
