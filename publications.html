---
layout: default
tags: home
---
<h1 id="publications">Research</h1>
<table cellspacing="0" cellpadding="0" class = "worktable">
  <thead>
    <tr>
      <th><b><a href="assets/ROMAN24.pdf">What Would I Do If...? Promoting Understanding in HRI through Real-Time Explanations in the Wild</a></b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tamlin Love, Antonio Andriella, Guillem Alenyà</td>
    </tr>
    <tr>
      <td><img src="img/ARI_Alberto_Speech.png" width="500"></td>
    </tr>
    <tr>
      <td><p>As robots become more and more integrated in human spaces, it is increasingly important for them to be able to explain their decisions to the people they interact with. These explanations need to be generated automatically and in real-time in response to decisions taken in dynamic and often unstructured environments. However, most research in explainable human-robot interaction only considers explanations (often manually selected) presented in controlled environments. We present an explanation generation method based on counterfactuals and demonstrate its use in an “in-the-wild” experiment using automatically generated and selected explanations of autonomous interactions with real people to assess the effect of these explanations on participants’ ability to predict the robot’s behaviour in hypothetical scenarios. Our results suggest that explanations aid one’s ability to predict the robot’s behaviour, but also that the addition of counterfactual statements may add some burden and counteract this beneficial effect.</p></td>
    </tr>
    <tr>
      <td></td>
    </tr>
    <tr>
      <td><i><a href="https://www.ro-man2024.org/">RO-MAN 2024</a></i></td>
    </tr>
    <tr>
      <td><a href="assets/ROMAN24.pdf" target="new">Paper</a> · <a href="https://github.com/tamlinlove/engage" target="new">Code</a></td>
    </tr>
  </tbody>
</table>
<table cellspacing="0" cellpadding="0" class = "worktable">
  <thead>
    <tr>
      <th><b><a href="assets/WARN2024.pdf">Personalising Explanations and Explaining Personalisation</a></b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tamlin Love, Antonio Andriella, Guillem Alenyà</td>
    </tr>
    <tr>
      <td><img src="img/WARN.png" width="500"></td>
    </tr>
    <tr>
      <td><p>Both personalisation and explainability have be-
        come popular research topics in social robotics, each capable
        of improving human-robot interactions. However, challenges
        have been identified in both fields, from issues of transparency,
        bias and privacy in personalisation to issues of identifying and
        communicating relevant explanations in explainability. In this
        work, we examine the intersection of these two fields - using
        personalisation to improve explanations and explainability to
        improve personalisation - and identify a number of research
        directions that could be of benefit to both communities.</p></td>
    </tr>
    <tr>
      <td></td>
    </tr>
    <tr>
      <td><i>To appear in <a href="https://warn-ws.github.io/index.html">WARN Workshop</a> @ <a href="https://www.ro-man2024.org/">RO-MAN 2024</a></i></td>
    </tr>
    <tr>
      <td><a href="assets/WARN2024.pdf" target="new">Paper</a></td>
    </tr>
  </tbody>
</table>
<table cellspacing="0" cellpadding="0" class = "worktable">
  <thead>
    <tr>
      <th><b><a href="https://dl.acm.org/doi/10.1145/3610978.3640734">Towards Explainable Proactive Robot Interactions for Groups of People in Unstructured Environments</a></b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tamlin Love, Antonio Andriella, Guillem Alenyà</td>
    </tr>
    <tr>
      <td><img src="img/HRI LBR Teaser Camera.png" width="500"></td>
    </tr>
    <tr>
      <td><p>For social robots to be able to operate in unstructured public spaces, they need to be able to gauge complex factors such as human-robot engagement and inter-person social groups, and be able to decide how and with whom to interact. Additionally, such robots should be able to explain their decisions after the fact, to improve accountability and confidence in their behavior. To address this, we present a two-layered proactive system that extracts high-level social features from low-level perceptions and uses these features to make high-level decisions regarding the initiation and maintenance of human robot interactions. With this system outlined, the primary focus of this work is then a novel method to generate counterfactual explanations in response to a variety of contrastive queries. We provide an early proof of concept to illustrate how these explanations can be generated by leveraging the two-layer system.</p></td>
    </tr>
    <tr>
      <td></td>
    </tr>
    <tr>
      <td><i>Late Breaking Report @ <a href="https://humanrobotinteraction.org/2024/">HRI 2024</a></i></td>
    </tr>
    <tr>
      <td><a href="assets/hri_lbr_24.pdf" target="new">Paper</a> · <a href="https://github.com/tamlinlove/engage" target="new">Code</a> · <a href="https://youtu.be/f9v1_iVIock" target="new">Video</a></td>
    </tr>
  </tbody>
</table>
<table cellspacing="0" cellpadding="0" class = "worktable">
  <thead>
    <tr>
      <th><b><a href="https://arxiv.org/pdf/2306.00035.pdf">ROSARL: Reward-Only Safe Reinforcement Learning</a></b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Geraud Nangue Tasse, Tamlin Love, Mark Nemecek, Steven James, Benjamin Rosman</td>
    </tr>
    <tr>
      <td><img src="img/rosarl_algorithm_trajs.png" width="500"></td>
    </tr>
    <tr>
      <td><p> An important problem in reinforcement learning is designing agents that learn to
        solve tasks safely in an environment. A common solution is for a human expert to
        define either a penalty in the reward function or a cost to be minimised when reach-
        ing unsafe states. However, this is non-trivial, since too small a penalty may lead
        to agents that reach unsafe states, while too large a penalty increases the time to
        convergence. Additionally, the difficulty in designing reward or cost functions can
        increase with the complexity of the problem. Hence, for a given environment with
        a given set of unsafe states, we are interested in finding the upper bound of rewards
        at unsafe states whose optimal policies minimises the probability of reaching those
        unsafe states, irrespective of task rewards. We refer to this exact upper bound as
        the Minmax penalty, and show that it can be obtained by taking into account both
        the controllability and diameter of an environment. We provide a simple practical
        model-free algorithm for an agent to learn this Minmax penalty while learning the
        task policy, and demonstrate that using it leads to agents that learn safe policies in
        high-dimensional continuous control environments.</p></td>
    </tr>
    <tr>
      <td></td>
    </tr>
    <tr>
      <td><i>Under review</i></td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/pdf/2306.00035.pdf" target="new">Paper</a> · <a href="https://github.com/geraudnt/rosarl" target="new">Code</a></td>
    </tr>
  </tbody>
</table>
<table cellspacing="0" cellpadding="0" class = "worktable">
  <thead>
    <tr>
      <th><b><a href="https://lifelongrobotics.github.io/#cfp">Facilitating Safe Sim-to-Real through Simulator Abstraction and Zero-shot Task Composition</a></b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tamlin Love, Devon Jarvis, Geraud Nangue Tasse, Branden Ingram, Steven James, Benjamin Rosman</td>
    </tr>
    <tr>
      <td><img src="img/sim2realtraj.png" width="500"></td>
    </tr>
    <tr>
      <td><p> Simulators are a fundamental part of training
        robots to solve complex control and navigation tasks. This
        is due to the speed and safety they offer in comparison
        to training directly on a physical system, where exploration
        may drive the system towards dangerous action for itself
        and its environment. However, simulators have a fundamental
        drawback known as the “reality gap”, which describes the
        discrepancy in performance which occurs when a robot trained
        in simulation performs the same task in the real world. The
        reality gap is prohibitive as it means many of the most powerful
        recent advances in reinforcement learning (RL) cannot be
        used with robots due to their high sample complexity which
        makes physical training infeasible. In this work we introduce a
        framework for applying high sample complexity RL algorithms
        to robots by leveraging recent advances in hierarchical RL and
        skill composition. We demonstrate that adapting hierarchical
        RL techniques allows us to close the reality gap at multiple
        levels of abstraction. As a result we are able to train a
        robot to perform combinatorially many tasks within a domain
        with minimal training on a physical system or steps of error
        correction. We believe this work provides an important starting
        framework for applying hierarchical RL to perform sim-to-real
        generalisation at multiple levels of abstraction.</p></td>
    </tr>
    <tr>
      <td></td>
    </tr>
    <tr>
      <td><i>Workshop on Lifelong Learning of High-level Cognitive and Reasoning Skills @ IROS, 2022</i></td>
    </tr>
    <tr>
      <td><a href="assets/Facilitating Safe Sim-to-Real through Simulator Abstraction.pdf" target="new">Paper</a> · <a href="https://www.youtube.com/watch?v=Xyx4S--BUCI" target="new">Video</a> · <a href="https://github.com/tamlinlove/kuricomposition" target="new">Code</a></td>
    </tr>
  </tbody>
</table>
<br>
<table cellspacing="0" cellpadding="0" class = "worktable">
  <thead>
    <tr>
      <th><b><a href="https://link.springer.com/article/10.1007/s00521-022-07808-y">Who should I trust? Cautiously learning with unreliable experts</a></b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tamlin Love, Ritesh Ajoodha and Benjamin Rosman</td>
    </tr>
    <tr>
      <td><img src="img/lwtt.png" width="500"></td>
    </tr>
    <tr>
      <td><p>An important problem in reinforcement learning is the need for greater sample efficiency. One approach to dealing with this problem is to incorporate external information elicited from a domain expert in the learning process. Indeed, it has been shown that incorporating expert advice in the learning process can improve the rate at which an agent’s policy converges. However, these approaches typically assume a single, infallible expert; learning from multiple and/or unreliable experts is considered an open problem in assisted reinforcement learning. We present CLUE (cautiously learning with unreliable experts), a framework for learning single-stage decision problems with action advice from multiple, potentially unreliable experts that augments an unassisted learning with a model of expert reliability and a Bayesian method of pooling advice to select actions during exploration. Our results show that CLUE maintains the benefits of traditional approaches when advised by reliable experts, but is robust to the presence of unreliable experts. When learning with multiple experts, CLUE is able to rank experts by their reliability and differentiate experts based on their reliability.</p></td>
    </tr>
    <tr>
      <td></td>
    </tr>
    <tr>
      <td><i>Neural Computing and Applications, 2022</i></td>
    </tr>
    <tr>
      <td><a href="assets/Who_Should_I_Trust.pdf" target="new">Paper</a> · <a href="assets/Who_Should_I_Trust_supp.pdf" target="new">Supplementary Material</a> · <a href="https://github.com/tamlinlove/CLUE_SSDP" target="new">Code</a></td>
    </tr>
  </tbody>
</table>
<br>
<table cellspacing="0" cellpadding="0" class = "worktable">
  <thead>
    <tr>
      <th><b>Learning Who to Trust: Policy Learning in Single-Stage Decision Problems with Unreliable Expert Advice</b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tamlin Love, Ritesh Ajoodha and Benjamin Rosman</td>
    </tr>
    <tr>
      <td><img src="img/thesis_diagram.png" width="500"></td>
    </tr>
    <tr>
      <td><p>Work in the field of Assisted Reinforcement Learning (ARL) has shown that the incorporation of external
        information in problem solving can greatly increase the rate at which learners can converge to an optimal
        policy and aid in scaling algorithms to larger, more complex problems. However, these approaches rely
        on a single, reliable source of information; the problem of learning with information from multiple
        and/or unreliable sources of information is still an open question in ARL.We present CLUE (Cautiously
        Learning with Unreliable Experts), a framework for learning single-stage decision problems with policy
        advice from multiple, potentially unreliable experts. We compare CLUE against an unassisted agent and
        an agent that na&#207;vely follows advice, and our results show that CLUE exhibits faster convergence than
        an unassisted agent when advised by reliable experts, but is nevertheless robust against incorrect advice
        from unreliable experts.</p></td>
    </tr>
    <tr>
      <td></td>
    </tr>
    <tr>
      <td><i>2022, MSc Thesis</i></td>
    </tr>
    <tr>
      <td><a href="assets/thesis.pdf" target="new">Thesis</a> · <a href="https://github.com/tamlinlove/CLUE_SSDP" target="new">Code</a></td>
    </tr>
    <tr>
      <td>Work published in <i>Neural Computing and Applications 2022</i></td>
    </tr>
    <tr>
      <td>Work featured in <i>Multi-disciplinary Conference on Reinforcement Learning and Decision Making 2022</i> (<a href="assets/RLDM_Extended_Abstract.pdf" target="new">Extended Abstract</a> · <a href="assets/posters/RLDM_2022_Poster.pdf" target="new">Poster</a>)</td>
    </tr>
    <tr>
      <td>Work featured in <i>Workshop on Human-aligned Reinforcement Learning for Autonomous Agents and Robots 2021</i> (<a href="assets/Should I Trust You.pdf" target="new">Paper</a> · <a href="https://www.youtube.com/watch?v=BybClvqoAwI" target="new">Video</a>)</td>
    </tr>
  </tbody>
</table>
<br>
<table cellspacing="0" cellpadding="0" class = "worktable">
<thead>
  <tr>
    <th><b><a href="https://ieeexplore.ieee.org/abstract/document/9040984">Building Undirected Influence Ontologies Using Pairwise Similarity Functions</a></b></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Tamlin Love and Ritesh Ajoodha</td>
  </tr>
  <tr>
    <td><img src="img/buio.png" width="500"></td>
  </tr>
  <tr>
    <td><p>The recovery of influence ontology structures is a
useful tool within knowledge discovery, allowing for an easy
and intuitive method of graphically representing the influences
between concepts or variables within a system. The focus of this
research is to develop a method by which undirected influence
structures, here in the form of undirected Bayesian network
skeletons, can be recovered from observations by means of
some pairwise similarity function, either a statistical measure
of correlation or some problem-specific measure.</p>
<p>In this research, we present two algorithms to construct
undirected influence structures from observations. The first
makes use of a threshold value to filter out relations denoting
weak influence, and the second constructs a maximum weighted
spanning tree over the complete set of relations. In addition, we
present a modification to the minimum graph edit distance (GED), which we refer to as the modified scaled GED, in order to
evaluate the performance of these algorithms in reconstructing
known structures. We perform a number of experiments in
reconstructing known Bayesian network structures, including a
real-world medical network. Our analysis shows that these
algorithms outperform a random reconstruction (modified scaled
GED ≈ 0.5), and can regularly achieve modified scaled GED
scores better than 0.3 in sparse cases and 0.45 in dense cases.</p>
<p>We argue that, while these methods cannot replace traditional
Bayesian network structure-learning techniques, they are useful
as computationally cheap data exploration tools and in knowledge
discovery over structures which cannot be modelled as Bayesian
networks.</p></td>
  </tr>
  <tr>
    <td></td>
  </tr>
  <tr>
    <td><i>2020 International SAUPEC/RobMech/PRASA Conference</i></td>
  </tr>
  <tr>
    <td><a href="assets/Building Undirected Influence Ontologies.pdf" target="new">Paper</a> · <a href="assets/posters/Honours_2019_Poster.pdf" target="new">Poster</a></td>
  </tr>
</tbody>
</table>
