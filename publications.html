---
layout: default
tags: home
---
<h1 id="publications">Research</h1>
<table cellspacing="0" cellpadding="0" class = "worktable">
  <thead>
    <tr>
      <th><b>Harnessing the wisdom of an unreliable crowd for autonomous decision making</b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tamlin Love, Ritesh Ajoodha and Benjamin Rosman</td>
    </tr>
    <tr>
      <td><img src="img/clue_rho_rldm.png" width="500"></td>
    </tr>
    <tr>
      <td><p>In Reinforcement Learning there is often a need for greater sample efficiency when learning an optimal policy, whether due to the complexity of the problem or the difficulty in obtaining data. One approach to tackling this problem is to introduce external information to the agent in the form of domain expert advice. Indeed, it has been shown that giving an agent advice in the form of state-action pairs during learning can greatly improve the rate at which the agent converges to an optimal policy. These approaches typically assume a single, infallible expert. However, it may be desirable to collect advice from multiple experts to further improve sample efficiency. This may introduce the problem of multiple experts offering conflicting advice. In general, experts (especially humans) can give incorrect advice. The problem of incorporating advice from multiple, potentially unreliable experts is considered an open problem in the field of Assisted Reinforcement Learning.</p>
        <p>Contextual bandits are an important class of problems with a broad range of applications such as in medicine, finance and recommendation systems. To address the problem of learning with expert advice from multiple, unreliable experts, we present CLUE (Cautiously Learning with Unreliable Experts), a framework which allows any contextual bandit algorithm to benefit from incorporating expert advice into its decision making. It does so by modelling the unreliability of each expert, and using this model to pool advice together to determine the probability of each action being optimal.</p>
        <p>We perform a number of experiments with simulated experts over randomly generated environments. Our results show that CLUE benefits from improved sample efficiency when advised by reliable experts, but is robust to the presence of unreliable experts, and is able to benefit from multiple experts. This research provides an approach to incorporating the advice of humans of varying levels of expertise in the learning process.</p></td>
    </tr>
    <tr>
      <td></td>
    </tr>
    <tr>
      <td><i>The Multi-disciplinary Conference on Reinforcement Learning and Decision Making, 2022 (Extended Abstract)</i></td>
    </tr>
    <tr>
      <td><a href="assets/RLDM_Extended_Abstract.pdf" target="new">Extended Abstract</a> · <a href="assets/posters/RLDM_2022_Poster.pdf" target="new">Poster</a></td>
    </tr>
  </tbody>
  </table>
  <br>
<table cellspacing="0" cellpadding="0" class = "worktable">
  <thead>
    <tr>
      <th><b>Learning Who to Trust: Policy Learning in Single-Stage Decision Problems with Unreliable Expert Advice</b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tamlin Love, Ritesh Ajoodha and Benjamin Rosman</td>
    </tr>
    <tr>
      <td><img src="img/thesis_diagram.png" width="500"></td>
    </tr>
    <tr>
      <td><p>Work in the field of Assisted Reinforcement Learning (ARL) has shown that the incorporation of external
        information in problem solving can greatly increase the rate at which learners can converge to an optimal
        policy and aid in scaling algorithms to larger, more complex problems. However, these approaches rely
        on a single, reliable source of information; the problem of learning with information from multiple
        and/or unreliable sources of information is still an open question in ARL.We present CLUE (Cautiously
        Learning with Unreliable Experts), a framework for learning single-stage decision problems with policy
        advice from multiple, potentially unreliable experts. We compare CLUE against an unassisted agent and
        an agent that na&#207;vely follows advice, and our results show that CLUE exhibits faster convergence than
        an unassisted agent when advised by reliable experts, but is nevertheless robust against incorrect advice
        from unreliable experts.</p></td>
    </tr>
    <tr>
      <td></td>
    </tr>
    <tr>
      <td><i>2022, MSc Thesis</i></td>
    </tr>
    <tr>
      <td><a href="assets/thesis.pdf" target="new">Thesis</a> · <a href="https://github.com/tamlinlove/CLUE_SSDP" target="new">Code</a></td>
    </tr>
  </tbody>
  </table>
  <br>
<table cellspacing="0" cellpadding="0" class = "worktable">
<thead>
  <tr>
    <th><b>Should I Trust You? Incorporating Unreliable Expert Advice in Human-Agent Interaction</b></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Tamlin Love, Ritesh Ajoodha and Benjamin Rosman</td>
  </tr>
  <tr>
    <td><img src="img/lwtt.png" width="500"></td>
  </tr>
  <tr>
    <td><p>A major concern in reinforcement learning, especially
as it is applied to real-world and robotics problems, is that
of sample-efficiency given increasingly complex problems and
the difficulty of data acquisition in certain domains. To that end,
many approaches incorporate external advice in the learning
process in order to increase the rate at which an agent learns to
solve a given problem. However, these approaches typically rely
on a single reliable information source; the problem of learning
with information from multiple, potentially unreliable sources
is still an open question in assisted reinforcement learning. We
present CLUE (Cautiously Learning with Unreliable Experts),
a framework for learning single-stage decision problems with
policy advice from multiple, potentially unreliable experts. We
compare CLUE against an unassisted agent and an agent that
na¨ıvely follows advice, and our results show that CLUE exhibits
faster convergence than an unassisted agent when advised by
reliable experts, but is nevertheless robust against incorrect
advice from unreliable experts.</p></td>
  </tr>
  <tr>
    <td></td>
  </tr>
  <tr>
    <td><i>2021, Workshop on Human-aligned Reinforcement Learning for Autonomous Agents and Robots</i></td>
  </tr>
  <tr>
    <td><a href="assets/Should I Trust You.pdf" target="new">Paper</a> · <a href="https://anonymous.4open.science/r/CLUE_SSDP-4425/README.md" target="new">Code</a></td>
  </tr>
</tbody>
</table>
<br>
<table cellspacing="0" cellpadding="0" class = "worktable">
<thead>
  <tr>
    <th><b>Building Undirected Influence Ontologies Using Pairwise Similarity Functions</b></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Tamlin Love and Ritesh Ajoodha</td>
  </tr>
  <tr>
    <td><img src="img/buio.png" width="500"></td>
  </tr>
  <tr>
    <td><p>The recovery of influence ontology structures is a
useful tool within knowledge discovery, allowing for an easy
and intuitive method of graphically representing the influences
between concepts or variables within a system. The focus of this
research is to develop a method by which undirected influence
structures, here in the form of undirected Bayesian network
skeletons, can be recovered from observations by means of
some pairwise similarity function, either a statistical measure
of correlation or some problem-specific measure.</p>
<p>In this research, we present two algorithms to construct
undirected influence structures from observations. The first
makes use of a threshold value to filter out relations denoting
weak influence, and the second constructs a maximum weighted
spanning tree over the complete set of relations. In addition, we
present a modification to the minimum graph edit distance (GED), which we refer to as the modified scaled GED, in order to
evaluate the performance of these algorithms in reconstructing
known structures. We perform a number of experiments in
reconstructing known Bayesian network structures, including a
real-world medical network. Our analysis shows that these
algorithms outperform a random reconstruction (modified scaled
GED ≈ 0.5), and can regularly achieve modified scaled GED
scores better than 0.3 in sparse cases and 0.45 in dense cases.</p>
<p>We argue that, while these methods cannot replace traditional
Bayesian network structure-learning techniques, they are useful
as computationally cheap data exploration tools and in knowledge
discovery over structures which cannot be modelled as Bayesian
networks.</p></td>
  </tr>
  <tr>
    <td></td>
  </tr>
  <tr>
    <td><i>2020 International SAUPEC/RobMech/PRASA Conference</i></td>
  </tr>
  <tr>
    <td><a href="assets/Building Undirected Influence Ontologies.pdf" target="new">Paper</a> · <a href="assets/posters/Honours_2019_Poster.pdf" target="new">Poster</a></td>
  </tr>
</tbody>
</table>
